import os
import shutil
import pandas as pd
import time
import threading
from keiri_project.config import TEMP_ROOT, OUTPUT_ROOT, COLUMN_MAP
from keiri_project.normalizer import normalize_name, normalize_store_name
from keiri_project.rules import (
    parse_date, normalize_receiver,
    classify_entry, normalize_identifier,
    log_uncertain_case
)
from keiri_project.logger import finalize_unmatched
from keiri_project.merge_summarize import merge_and_aggregate  # ğŸ”„ çµ±åˆå‡¦ç†ã‚’å‘¼ã³å‡ºã—
from keiri_project.parser import parse_filename
from keiri_project.transformer import transform_file  # ä»®æƒ³ã®å‡¦ç†é–¢æ•°

LAST_PROCESSED_FILE = os.path.join(OUTPUT_ROOT, "_last_updated.txt")


def process_group(dept: str, yyyymm: str):
    temp_dir = os.path.join(TEMP_ROOT, dept, yyyymm)
    if not os.path.isdir(temp_dir):
        return
    output_dir = os.path.join(OUTPUT_ROOT, dept, yyyymm)
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir, exist_ok=True)

    frames = []
    for fname in sorted(os.listdir(temp_dir)):
        path = os.path.join(temp_dir, fname)
        if fname.lower().endswith(('.xlsx', '.xls')):
            sheets = pd.read_excel(path, sheet_name=None)
            for sheet_name, df in sheets.items():
                df['__source_file'] = fname
                df['__sheet'] = sheet_name
                frames.append(df)
        elif fname.lower().endswith('.csv'):
            df = pd.read_csv(path, encoding='utf-8')
            df['__source_file'] = fname
            df['__sheet'] = ''
            frames.append(df)
    if not frames:
        return

    df_all = pd.concat(frames, ignore_index=True)
    df_all.columns = df_all.columns.str.strip().str.replace(r"[\n\r\t\u3000]", "", regex=True)

    df_all['éƒ¨ç½²å'] = dept

    date_col = next((c for c in df_all.columns if 'æ—¥' in c and c != 'éƒ¨ç½²å'), None)
    df_all['æ—¥ä»˜'] = df_all[date_col].astype(str).apply(lambda x: parse_date(x, fallback_year=int(yyyymm[:4])))

    name_keys = COLUMN_MAP['ä¼æ¥­å']
    name_col = next((c for c in df_all.columns if any(k in c for k in name_keys)), None)
    df_all['ä¼æ¥­å'] = df_all[name_col].astype(str).apply(
        lambda x: normalize_name(x, df_all['__source_file'].iloc[0], name_col, output_dir)
    ) if name_col else ''

    store_keys = COLUMN_MAP['åº—èˆ—å']
    store_col = next((c for c in df_all.columns if any(k in c for k in store_keys)), None)
    df_all['åº—èˆ—å'] = df_all[store_col].astype(str).apply(
        lambda x: normalize_store_name(x, df_all['__source_file'].iloc[0], store_col, output_dir)
    ) if store_col else ''

    df_all['é€ã‚Šå…ˆ'] = df_all['åº—èˆ—å'].apply(normalize_receiver)

    amt_keys = COLUMN_MAP['é‡‘é¡']
    amt_col = next((c for c in df_all.columns if any(k in c for k in amt_keys)), None)
    df_all['å£²ä¸Šé«˜'] = pd.to_numeric(df_all[amt_col], errors='coerce').fillna(0).astype(int) if amt_col else 0

    prod_keys = COLUMN_MAP['å•†å“å']
    prod_col = next((c for c in df_all.columns if any(k in c for k in prod_keys)), None)
    if prod_col:
        raw_names = df_all[prod_col].astype(str)
        classified = raw_names.apply(classify_entry)
        df_all[['å•†å“å', 'ä½œæ¥­é …ç›®']] = pd.DataFrame(list(classified), index=df_all.index)
    else:
        df_all['å•†å“å'] = ''
        df_all['ä½œæ¥­é …ç›®'] = ''

    num_keys = COLUMN_MAP['æ•°é‡']
    num_col = next((c for c in df_all.columns if any(k in c for k in num_keys)), None)
    df_all['æ•°é‡'] = pd.to_numeric(df_all[num_col], errors='coerce').fillna(0).astype(int) if num_col else 0

    price_keys = COLUMN_MAP['å˜ä¾¡']
    price_col = next((c for c in df_all.columns if any(k in c for k in price_keys)), None)
    df_all['å˜ä¾¡'] = pd.to_numeric(df_all[price_col], errors='coerce').fillna(0).astype(int) if price_col else 0

    df_all.loc[(df_all['ä½œæ¥­é …ç›®'] != '') & (df_all['æ•°é‡'] == 0), 'æ•°é‡'] = 1

    def set_classification(row):
        if row['å•†å“å']:
            return 'å•†å“'
        if row['ä½œæ¥­é …ç›®']:
            return 'ä½œæ¥­'
        return ''
    df_all['åˆ†é¡'] = df_all.apply(set_classification, axis=1)

    for id_col in ['ä¼ç¥¨ç•ªå·', 'æ³¨æ–‡ç•ªå·']:
        if id_col in df_all.columns:
            df_all[id_col] = df_all[id_col].apply(normalize_identifier)

    df_all.apply(lambda row: log_uncertain_case(row, 'è¦GPTåˆ†é¡') if row['ä½œæ¥­é …ç›®'] == 'è¦GPTåˆ†é¡' else None, axis=1)

    fields = [
        'éƒ¨ç½²å', 'æ—¥ä»˜', 'ä¼æ¥­å', 'é€ã‚Šå…ˆ',
        'å•†å“å', 'ä½œæ¥­é …ç›®', 'åˆ†é¡', 'æ•°é‡', 'å˜ä¾¡', 'å£²ä¸Šé«˜', 'æ³¨æ–‡ç•ªå·', 'ä¼ç¥¨ç•ªå·'
    ]
    rec = df_all[fields]
    rec.to_csv(os.path.join(output_dir, 'records.csv'), index=False)
    rec.to_excel(os.path.join(output_dir, 'records.xlsx'), index=False)
    rec.to_csv(os.path.join(output_dir, 'summary.csv'), index=False)
    rec.to_excel(os.path.join(output_dir, 'summary.xlsx'), index=False)

    finalize_unmatched(output_dir)

    # âœ… çµ±åˆç”¨ã®ã€Œå‡¦ç†æ™‚é–“ãƒ•ã‚¡ã‚¤ãƒ«ã€ã‚’æ›´æ–°
    with open(LAST_PROCESSED_FILE, 'w') as f:
        f.write(str(time.time()))


def start_merge_summary_watcher(interval_sec=5, cooldown_sec=15):
    """
    OUTPUT_ROOT/_last_updated.txt ã®æ›´æ–°ã‚’ç›£è¦–ã—ã€
    æœ€å¾Œã®å‡¦ç†ã‹ã‚‰ cooldown_sec ç§’çµŒéã—ãŸã‚‰ summary ã‚’è‡ªå‹•çµ±åˆã€‚
    """
    def watcher():
        last_merged = 0
        ts_path = LAST_PROCESSED_FILE
        while True:
            try:
                if os.path.exists(ts_path):
                    with open(ts_path, 'r') as f:
                        ts = float(f.read().strip())
                    now = time.time()
                    if ts > last_merged and (now - ts >= cooldown_sec):
                        print("ğŸŒ€ çµ±åˆsummaryã‚’è‡ªå‹•å†ç”Ÿæˆã—ã¾ã™...")
                        merge_and_aggregate()
                        last_merged = now
            except Exception as e:
                print(f"âš ï¸ è‡ªå‹•çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            time.sleep(interval_sec)

    threading.Thread(target=watcher, daemon=True).start()


def handle_new_file(file_path):
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰éƒ¨ç½²åãƒ»å¹´æœˆãƒ»ä¼æ¥­åãªã©ã‚’æŠ½å‡ºã—ã¦å‡¦ç†é–‹å§‹
    print(f"ğŸ›  å‡¦ç†é–‹å§‹: {file_path}")
    file_info = parse_filename(file_path)
    if file_info is None:
        print("âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«åãŒä¸æ­£ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
        return
    transform_file(file_path, file_info)
